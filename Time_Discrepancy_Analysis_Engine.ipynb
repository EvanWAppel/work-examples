{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time Discrepancy Analysis Engine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvanWAppel/work-examples/blob/main/Time_Discrepancy_Analysis_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNDOdMzRq-Hu"
      },
      "source": [
        "# Time Discrepancy Analysis Engine\n",
        "\n",
        "This Python notebook is designed to compile and analyze several sets of data and to produce tools for discovering discrepancies.\n",
        "\n",
        "When run, the engine will produce a set of excel files with findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKBHz59czi0Q"
      },
      "source": [
        "## Step 1:\n",
        "\n",
        "Upload data to content folder in filesystem.\n",
        "\n",
        "Files need to be named thusly, followed by the period of time:\n",
        "\n",
        "1.   employee_file, N/A\n",
        "2.   exceptions, First day of last pay period to yesterday.\n",
        "3.   schedule, First day of last pay period to last day of next pay period.\n",
        "4.   project, First day of last pay period to yesterday.\n",
        "5.   availability, First day of last pay period to last day of next pay period.\n",
        "6.   scores, First day of last pay period to yesterday.\n",
        "7.   Variance, Past 30 days.\n",
        "8.   sessions, First day of last pay period to yesterday.\n",
        "9.   slices, Past seven days.\n",
        "\n",
        "## Step 2:\n",
        "\n",
        "Run the Engine. Click \"Runtime\" in the toolbar. Then click \"Run All.\"\n",
        "\n",
        "## Step 3: \n",
        "\n",
        "Download the file.\n",
        "\n",
        "## More information?\n",
        "\n",
        "For more information, please see the following cell which details specifications. If you can't find what you're looking for, please call Evan Appel at 702-466-4498.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsKChydQ2oS"
      },
      "source": [
        "#Query References\n",
        "##Sessions Query\n",
        "\n",
        "```\n",
        "\n",
        " /*TITLE: Agent Sessions Query\n",
        "   AUTHOR: Evan Appel\n",
        "   UPDATED: May 7, 2020*/\n",
        " \n",
        "        SELECT LOWER(a.FirstName + ' ' + a.LastName) AS agent\n",
        "             , LEFT(dirsrv.text,PATINDEX('% %',dirsrv.text)) AS bamboo_id\n",
        "             , o2.Name AS project_name\n",
        "             -- The following sets the login and logout times to ISO 8601 for coding purposes\n",
        "             /*, FORMAT(s.loginDateTime,'yyyy-MM-ddTHH:mm:ss.fffZ') AS LOGIN\n",
        "             , FORMAT(s.logoutDateTime, 'yyyy-MM-ddTHH:mm:ss.fffZ') AS logout */\n",
        "             , CAST(DATEADD(ss,DATEDIFF(ss,GETUTCDATE(), GETDATE()),s.loginDateTime) AS DATE) AS LOGIN_DATE\n",
        "             , CAST(DATEADD(ss,DATEDIFF(ss,GETUTCDATE(), GETDATE()),s.loginDateTime) AS TIME) AS lOGIN_TIME\n",
        "             , CAST(DATEADD(ss,DATEDIFF(ss,GETUTCDATE(), GETDATE()),s.logoutDateTime) AS DATE) AS LOGOUT_DATE\n",
        "             , CAST(DATEADD(ss,DATEDIFF(ss,GETUTCDATE(), GETDATE()),s.logoutDateTime) AS TIME) AS LOGOUT_TIME\n",
        "             , DATEDIFF(ms, s.loginDateTime, s.logoutDateTime)/1000 AS sesh_dur\n",
        "             , [talkTime]\n",
        "             , [waitingTime]\n",
        "             , [pauseTime]\n",
        "             , [reviewTime]\n",
        "             , [talkTime] + [waitingTime] + [pauseTime] + [reviewTime] AS soat \n",
        "            -- , ap.total AS completes\n",
        "             --, (((DATEDIFF(ms, s.loginDateTime, s.logoutDateTime)/1000)  / ap.total) / 60) AS prod_rate\n",
        "           /* */\n",
        "          FROM [VoxcoSystem].[dbo].[AgentSession] s\n",
        "     LEFT JOIN [VoxcoSystem].[dbo].[tblAgents] a \n",
        "            ON s.userId = a.k_Id\n",
        "     LEFT JOIN [VoxcoSystem].[dbo].[tblObjects] o \n",
        "            ON s.projectId = o.k_Id\n",
        "     LEFT JOIN [VoxcoSystem].[dbo].[tblObjects] o2 \n",
        "            ON o.ParentId = o2.k_Id\n",
        "     LEFT JOIN [VoxcoSystem].[dbo].[History_DirSrvObjName] dirsrv\n",
        "            ON s.userId = dirsrv.id\n",
        "     LEFT JOIN [VoxcoSystem].[dbo].[AgentSessionConnectionTimes] c \n",
        "            ON s.sessionId = c.sessionId\n",
        "     --LEFT JOIN (SELECT * FROM [VoxcoSystem].[dbo].[AgentSessionProductivity]\n",
        "               --  WHERE resultCode = 'CO') ap\n",
        "           -- ON s.sessionId = ap.sessionId\n",
        "            -- Date parameter set to yesterday, local time.\n",
        "         WHERE CAST(DATEADD(ss,DATEDIFF(ss,GETUTCDATE(), GETDATE()),s.loginDateTime) AS DATE) BETWEEN '2021-08-28' AND '2021-09-03' -- weekend code\n",
        "        -- CAST(DATEADD(ss,DATEDIFF(ss,GETUTCDATE(), GETDATE()),s.loginDateTime) AS DATE) = CAST(getdate() - 1 AS DATE) \n",
        "          -- and a.FirstName = 'richard'\n",
        "      ORDER BY loginDateTime DESC\n",
        "```\n",
        "##Slices Query\n",
        "Use in Cloudwatch, prod event bus\n",
        "\n",
        "```\n",
        "fields time, detail.associate, detail.start, detail.end, detail.minutes, detail.punch, detail.project, `detail.slice-type`\n",
        "| filter (`detail-type` = \"slice\") \n",
        "and `detail.slice-type` not like \"epoch\" \n",
        "and `detail.punch` not in [\"out\",\"manager\",\"courtesy-patrol\",\"monitoring\",\"team-lead-assistant\",\"office-maintenance\",\"office-technician\",\"office-admin\",\"trainer\",\"quota-managemeent\",\"\"]\n",
        "and strlen(`detail.punch`) >=1\n",
        "and `detail.project` not like 'nis'\n",
        "| sort @timestamp desc\n",
        "|limit 10000\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcfO9P0KClv5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ2iMHwoCSmc"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from datetime import date\n",
        "import pytz \n",
        "from dateutil import parser\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "%load_ext google.colab.data_table\n",
        "from google.colab import data_table\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juxubUrIhGeG"
      },
      "source": [
        "# Options and Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM1Z24EiatFN"
      },
      "source": [
        "#Sets options so the data frame doesn't break into \"pages\"\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 800)\n",
        "# When I assign a column to itself to change its datatype to datetime it throws a warning, this suppresses that warning. #### USE EXTREME CAUTION\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPoV9plC1ha"
      },
      "source": [
        "# Ingesters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHQtYLoFC8iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdda1ed-04bf-4632-c0aa-f76525744a52"
      },
      "source": [
        "# Employee Data\n",
        "try:\n",
        "  employeeFile = pd.read_csv('employee_file.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"No Employee File Available\")\n",
        "else:\n",
        "  print(\"Employee File checker ...\")\n",
        "  checker = 0\n",
        "  if len(employeeFile) > 0:\n",
        "    print(\"Employee File is non-empty. \"+str(len(employeeFile))+\" records found.\")\n",
        "  else:\n",
        "    print(\"Employee File problem: Empty File! \"+str(len(employeeFile))+\" records found.\")\n",
        "  for i in employeeFile.columns :\n",
        "    if i in ['Last name, First name', 'Employee #', 'Status', 'Division', 'Job Title','Hire Date']:\n",
        "      checker += 1\n",
        "  if checker == 6:\n",
        "    print(\"Required columns present for Employee File\")\n",
        "  else:\n",
        "    print(\"Check Employee File document for correct fields, referenced above.\")\n",
        "\n",
        "# Exception Data\n",
        "try:\n",
        "  exceptions = pd.read_csv('exceptions.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"No exceptions file Available\")\n",
        "else:\n",
        "  print(\"Exceptions file checker ...\")\n",
        "  if len(exceptions) > 0:\n",
        "    print(\"Employee File is non-empty. \"+str(len(exceptions))+\" reecords found.\")\n",
        "  else:\n",
        "    print(\"Employee File is empty! \"+str(len(exceptions))+\" records found.\")\n",
        "  print(\"Data begins on \"+str(exceptions['date'].min())+\"\\n\"+\"And ends on \"+str(exceptions['date'].max()))\n",
        "\n",
        "# Schedule Data\n",
        "try:\n",
        "  schedule = pd.read_csv('schedule.csv')\n",
        "except:\n",
        "  print(\"No schedule file Available\")\n",
        "else:\n",
        "  print(\"Schedule file checker ...\")\n",
        "  if len(schedule) > 0:\n",
        "    print(\"Schedule File is non-empty. \"+str(len(schedule))+\" reecords found.\")\n",
        "  else:\n",
        "    print(\"Schedule File is empty! \"+str(len(schedule))+\" records found.\")\n",
        "  print(\"Data begins on \"+str(schedule['Date'].min())+\"\\n\"+\"And ends on \"+str(schedule['Date'].max()))\n",
        "# Sessions Data\n",
        "try:\n",
        "  sessions = pd.read_excel('sessions.xlsx', parse_dates=True)\n",
        "except:\n",
        "  print(\"No sessions file Available\")\n",
        "else:\n",
        "  print(\"Sessions file checker ...\")\n",
        "  if len(sessions) > 0:\n",
        "    print(\"Sessions File is non-empty. \"+str(len(sessions))+\" records found.\")\n",
        "  else:\n",
        "    print(\"Sessions File is empty! \"+str(len(sessions))+\" records found.\")\n",
        "  print(\"Data begins on \"+str(sessions['LOGIN_DATE'].min())+\"\\n\"+\"And ends on \"+str(sessions['LOGIN_DATE'].max()))\n",
        "# Slices Data\n",
        "try:\n",
        "  slices = pd.read_csv('slices.csv', parse_dates=True)\n",
        "except:\n",
        "  print(\"No slices file Available\")\n",
        "else:\n",
        "  print(\"Slices file checker ...\")\n",
        "  if len(slices) > 0:\n",
        "    print(\"Slices File is non-empty. \"+str(len(slices))+\" records found.\")\n",
        "  else:\n",
        "    print(\"Slices File is empty! \"+str(len(schedule))+\" records found.\")\n",
        "  print(\"Data begins on \"+str(slices['detail.start'].min())+\"\\n\"+\"And ends on \"+str(slices['detail.start'].max()))\n",
        "# Project Data\n",
        "try:\n",
        "  project = pd.read_csv('project.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"no project file available\")\n",
        "else:\n",
        "  print(\"Project file checker...\")\n",
        "  if (len(project)>0):\n",
        "    print(\"Project file is non-empty. \"+str(len(project))+\" records found.\")\n",
        "# Variance Data\n",
        "try: \n",
        "  variance = pd.read_csv('Variance.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"no variance file available\")\n",
        "else:\n",
        "  print(\"variance file checker...\")\n",
        "  if len(variance)>0:\n",
        "    print(\"Variance file is non-empty. \"+str(len(variance))+\" records found.\")\n",
        "# availability Data\n",
        "try: \n",
        "  availability = pd.read_csv('availability.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"no availability file available\")\n",
        "else:\n",
        "  print(\"availability file checker...\")\n",
        "  if len(availability)>0:\n",
        "    print(\"availability file is non-empty. \"+str(len(availability))+\" records found.\")\n",
        "# Scores data\n",
        "try: \n",
        "  scores = pd.read_csv('scores.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"no scores file available\")\n",
        "else:\n",
        "  print(\"scores file checker...\")\n",
        "  if len(scores)>0:\n",
        "    print(\"scores file is non-empty. \"+str(len(scores))+\" records found.\")\n",
        "# Requested days off\n",
        "try: \n",
        "  requests = pd.read_csv('requests.csv',parse_dates=True)\n",
        "except:\n",
        "  print(\"no requests file available\")\n",
        "else:\n",
        "  print(\"requests file checker...\")\n",
        "  if len(scores)>0:\n",
        "    print(\"requests file is non-empty. \"+str(len(requests))+\" records found.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Employee File Available\n",
            "No exceptions file Available\n",
            "No schedule file Available\n",
            "No sessions file Available\n",
            "No slices file Available\n",
            "no project file available\n",
            "no variance file available\n",
            "no availability file available\n",
            "no scores file available\n",
            "no requests file available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = pd.read_csv('exceptions.csv')\n",
        "print(e.head())"
      ],
      "metadata": {
        "id": "ebGwXD8vepb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062aa850-850f-4b3e-f3d7-67505db57095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  id     eid               name  supervisor                      created     date assignee associate-status approval-status  approver approvalTS resolved-status  resolver resolvedTS       project commenters\n",
            "0  exception:26010_2022-02-20:9be56f2f1fc64528bae...   26010      Lisa Kuntzman     35400.0  2022-02-20T22:03:17.087303Z  2/20/22    35400         APPROVED            NONE       NaN        NaN            NONE       NaN        NaN       s2r1001        NaN\n",
            "1  exception:105829_2022-02-20:b8671eac74624451bd...  105829      Latanya Mason     34688.0  2022-02-21T10:41:33.476026Z  2/20/22   105829             NONE            NONE       NaN        NaN            NONE       NaN        NaN  cf1009w-cell        NaN\n",
            "2  exception:105691_2022-02-20:a74a31b61410402bbf...  105691    Scott Lefkowitz     34688.0  2022-02-20T19:47:23.847004Z  2/20/22    34688         APPROVED            NONE       NaN        NaN            NONE       NaN        NaN  cf1009w-cell        NaN\n",
            "3  exception:105484_2022-02-20:d793609e08aa4261b7...  105484        Joshua Hull     34688.0  2022-02-20T22:13:05.703938Z  2/20/22    34688         APPROVED            NONE       NaN        NaN            NONE       NaN        NaN         staff        NaN\n",
            "4  exception:101367_2022-02-20:a9bd2dffefa54be29a...  101367  Michele Jankowiak     35400.0  2022-02-20T21:47:59.687560Z  2/20/22    35400         APPROVED            NONE       NaN        NaN            NONE       NaN        NaN  cf1009w-cell        NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5_uBkkTiT9O"
      },
      "source": [
        "# Table Preparation\n",
        "This part is to get table data types arranged properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owei9F9RSmY7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "7275f9a1-fa7b-4bcc-b585-c39f9d13e8fe"
      },
      "source": [
        "### Employee File\n",
        "# Only Relevant Columns\n",
        "employee = employeeFile[[\"Last name, First name\",\"Employee #\",\"Status\",\"Division\",\"Job Title\",\"Hire Date\"]]\n",
        "# Better column names\n",
        "employee = employee.rename(columns={\"Last name, First name\":\"full_name\",\n",
        "                                    \"Employee #\": \"eid\",\n",
        "                                    \"Hire Date\": \"hire_date\"})\n",
        "# Set hire date to proper date type\n",
        "employee[\"hire_date\"] = pd.to_datetime(employee[\"hire_date\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c4f7ef698ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Employee File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Only Relevant Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0memployee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memployeeFile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Last name, First name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Employee #\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Status\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Division\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Job Title\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hire Date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Better column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m employee = employee.rename(columns={\"Last name, First name\":\"full_name\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'employeeFile' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoNRCQdmIJZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "895e9bee-b725-45da-bf62-1223f49c8203"
      },
      "source": [
        "### Exceptions\n",
        "# Only Relevant columns\n",
        "excep = exceptions[[\"eid\",\"date\",\"project\",\"id\"]]\n",
        "# Set Date to proper type\n",
        "excep[\"date\"] = pd.to_datetime(excep[\"date\"])\n",
        "# Aggregate\n",
        "excep = excep.groupby([\"eid\",\"date\",\"project\"], as_index=False)[\"id\"].size().fillna(0)\n",
        "excep = excep.rename(columns={\"size\":\"exception_count\"})\n",
        "excep = excep[[\"eid\",\"date\",\"exception_count\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5c7908bae9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Only Relevant columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexcep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"project\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set Date to proper type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexcep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exceptions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEny8fPYYhry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "8d7297dc-1949-41a3-c03a-420f5186e2e3"
      },
      "source": [
        "### Schedule\n",
        "# Only Relevant Columns\n",
        "sched = schedule[[\"EID\",\"Project Name\",\"Date\",\"StartTime\",\"EndTime\"]]\n",
        "# column names are to be descriptive, undercase and words separated by underscores\n",
        "sched = sched.rename(columns={\"EID\":\"eid\",\n",
        "                              \"Project Name\":\"sched_project\",\n",
        "                              \"Date\":\"date\",\n",
        "                              \"StartTime\":\"sched_start\",\n",
        "                              \"EndTime\":\"sched_end\"})\n",
        "# Convert the datetimes to proper types. WARNING: Had to disable a warning for the following 3 rows. Note in above section.\n",
        "sched[\"sched_start\"] = pd.to_datetime(sched[\"date\"] + \" \" + sched[\"sched_start\"])\n",
        "sched[\"sched_end\"] = pd.to_datetime(sched[\"date\"] + \" \" + sched[\"sched_end\"])\n",
        "sched[\"date\"] = pd.to_datetime(sched[\"date\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-376dba644c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Only Relevant Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Project Name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"StartTime\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"EndTime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# column names are to be descriptive, undercase and words separated by underscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sched = sched.rename(columns={\"EID\":\"eid\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'schedule' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ62fL-ZZ7bX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "51136e6c-0053-4b3d-8874-4d4c2b5f5648"
      },
      "source": [
        "### Sessions\n",
        "# Relevant Columns\n",
        "sesh = sessions[[\"bamboo_id\",\"project_name\",\"LOGIN_DATE\",\"lOGIN_TIME\",\"LOGOUT_DATE\",\"LOGOUT_TIME\",\"sesh_dur\",\"talkTime\",\"waitingTime\",\"pauseTime\",\"reviewTime\",\"soat\"]]\n",
        "# Rename Columns\n",
        "sesh = sesh.rename(columns={\"bamboo_id\":\"eid\",\n",
        "                            \"project_name\":\"vc_project\",\n",
        "                            \"talkTime\":\"talk_time\",\n",
        "                            \"waitingTime\":\"waiting_time\",\n",
        "                            \"pauseTime\":\"pause_time\",\n",
        "                            \"reviewTime\":\"review_time\"})\n",
        "# Correct Datatypes\n",
        "# To get the dates working: basically, the date and time need to be converted to strings and then concatenated, from there, a UTC suffix can be added with the first tz_localize, converted to Pacific time, and then remove the suffix with the second tz_localize\n",
        "sesh[\"vc_login\"] = pd.to_datetime(sesh[\"LOGIN_DATE\"].astype(str) + \" \" + sesh['lOGIN_TIME'].astype(str)).dt.tz_localize(\"US/Pacific\").dt.tz_localize(None)\n",
        "sesh[\"LOGOUT_DATE\"] = sesh[\"LOGOUT_DATE\"].fillna(\"2021-10-14\")\n",
        "sesh['LOGOUT_TIME'] = sesh['LOGOUT_TIME'].fillna(\"10:50:00\")\n",
        "sesh[\"vc_logout\"] = pd.to_datetime(sesh[\"LOGOUT_DATE\"].astype(str) + \" \" + sesh['LOGOUT_TIME'].astype(str)).dt.tz_localize(\"US/Pacific\").dt.tz_localize(None)\n",
        "sesh[\"date\"] = sesh[\"vc_login\"].dt.date\n",
        "sesh[\"eid\"] = sesh[\"eid\"].fillna(0)\n",
        "sesh = sesh[sesh[\"eid\"] != '26045B']\n",
        "sesh = sesh[sesh[\"eid\"] != 'jleejoyce']\n",
        "sesh = sesh[sesh[\"eid\"] != 'kbigelow']\n",
        "sesh = sesh[sesh[\"eid\"] != 'tstrauss']\n",
        "sesh[\"eid\"] = sesh[\"eid\"].astype(\"str\").astype(\"int64\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d83a8b65172b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Relevant Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bamboo_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"project_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LOGIN_DATE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lOGIN_TIME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LOGOUT_DATE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LOGOUT_TIME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sesh_dur\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"talkTime\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"waitingTime\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pauseTime\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reviewTime\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"soat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Rename Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sesh = sesh.rename(columns={\"bamboo_id\":\"eid\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sessions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD6fD0vtV9Kf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "5880f722-49ff-4349-8afc-113878fc828a"
      },
      "source": [
        "### Slices\n",
        "# Relevant Columns\n",
        "sl = slices[[\"time\",\"detail.associate\",\"detail.start\",\"detail.end\",\"detail.minutes\",\"detail.punch\",\"detail.project\"]]\n",
        "# Rename Columns\n",
        "sl = sl.rename(columns={\"time\":\"timestamp\",\n",
        "                        \"detail.associate\":\"eid\",\n",
        "                        \"detail.start\":\"ontime_start\",\n",
        "                        \"detail.end\":\"ontime_end\",\n",
        "                        \"detail.minutes\":\"ontime_duration\",\n",
        "                        \"detail.punch\":\"punch\",\n",
        "                        \"detail.project\":\"ontime_project\"})\n",
        "# Data Type Correction\n",
        "sl[\"eid\"] = sl[\"eid\"].astype(\"int64\")\n",
        "sl[\"ontime_start\"] = pd.to_datetime(sl[\"ontime_start\"]).dt.tz_convert(\"US/Pacific\").dt.tz_localize(None)\n",
        "sl[\"ontime_end\"] = pd.to_datetime(sl[\"ontime_end\"]).dt.tz_convert(\"US/Pacific\").dt.tz_localize(None)\n",
        "sl[\"date\"] = sl[\"ontime_start\"].dt.date\n",
        "sl[\"timestamp\"] = pd.to_datetime(sl[\"timestamp\"])\n",
        "# Data Cleaning\n",
        "# Rows with NAN in certain columns indicate a correction that hasn't been filtered by Cloudwatch\n",
        "sl = sl.dropna()\n",
        "# removing corrections\n",
        "# The problem is that there are duplicates here. Records that have the same start or end times. \n",
        "# The younger one is the correct one.\n",
        "# so, assign them a rank where we look at each start or end time and start counting up backwards\n",
        "# if there's only one record with that timestamp then it gets a one\n",
        "# if there's a duplicate start time then the program looks at the timestamp and sees which one is older.\n",
        "# The oldeest timestamp gets the 1, which ensures when we filter only the rows with a rank of 1\n",
        "# we will have the latest slices and no corrections.\n",
        "# It occurs to me that the rank sections should probably group by eid too to avoid \n",
        "# Duplicate timestamps between people, but the precision on these timestamps is such that I think it\n",
        "# Very unlikely that anything but a system operation would end up producing duplicate timestamps\n",
        "cs = sl[[\"timestamp\",\n",
        "         \"eid\",\n",
        "         \"ontime_start\",\n",
        "         \"ontime_end\",\n",
        "         \"ontime_duration\",\n",
        "         \"punch\",\n",
        "         \"ontime_project\",\n",
        "         \"date\"]]\n",
        "ce = sl[[\"timestamp\",\n",
        "         \"eid\",\n",
        "         \"ontime_start\",\n",
        "         \"ontime_end\",\n",
        "         \"ontime_duration\",\n",
        "         \"punch\",\n",
        "         \"ontime_project\",\n",
        "         \"date\"]]\n",
        "\n",
        "cs =(\n",
        "        cs.assign(\n",
        "            rnk=cs.groupby([\"ontime_start\"])[\"timestamp\"].rank(\n",
        "                method=\"first\", ascending=False\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "ce =(\n",
        "        ce.assign(\n",
        "            rnk=ce.groupby([\"ontime_start\"])[\"timestamp\"].rank(\n",
        "                method=\"first\", ascending=False\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "sl = pd.concat([cs,ce])\n",
        "sl = sl[sl[\"rnk\"]==1]\n",
        "sl = sl.drop_duplicates()\n",
        "sl[\"timestamp\"] = pd.to_datetime(sl[\"timestamp\"]).dt.tz_localize(None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1a881ff43a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Relevant Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"detail.associate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"detail.start\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"detail.end\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"detail.minutes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"detail.punch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"detail.project\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Rename Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sl = sl.rename(columns={\"time\":\"timestamp\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'slices' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRtTAcJx10M3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "33e1a70a-be74-42bb-84e7-9fba41e6a1e5"
      },
      "source": [
        "### Project\n",
        "# For total Ontime minutes in metrics\n",
        "project[\"date\"] = pd.to_datetime(project[\"date\"])\n",
        "ontMin = project.groupby([\"EID\",\"date\"],as_index=False).agg({\"hours\":sum})\n",
        "ontMin = ontMin.assign(ontimeDuration=ontMin[\"hours\"]*60)\n",
        "\n",
        "ontMin = ontMin[[\"EID\",\"date\",\"ontimeDuration\"]]\n",
        "ontMin = ontMin.rename(columns={\"EID\":\"eid\",\n",
        "                                \"ontimeDuration\": \"ontime_duration\"})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0d0c060c951b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# For total Ontime minutes in metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mproject\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0montMin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"hours\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0montMin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0montMin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0montimeDuration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0montMin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hours\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'project' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e49r1_c3AWlB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "aa072f2c-272f-45e5-de7f-18cdac9854b4"
      },
      "source": [
        "# availability\n",
        "availability[\"date\"] = pd.to_datetime(availability[\"date\"]).dt.date\n",
        "availability[\"avail_start\"] = pd.to_datetime(availability[\"date\"].astype(\"str\")+\" \"+availability[\"start\"])\n",
        "availability[\"avail_end\"] = pd.to_datetime(availability[\"date\"].astype(\"str\")+\" \"+availability[\"end\"])\n",
        "availability[\"date\"] = pd.to_datetime(availability[\"date\"])\n",
        "avail = availability[[\"date\",\"eid\",\"avail_start\",\"avail_end\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e8d30cbf534e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# availability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avail_start\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avail_end\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'availability' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhlY4O6ZAW0X"
      },
      "source": [
        "# variance\n",
        "excuses = variance[[\"EID\",\"date\",\"absent\"]]\n",
        "excuses = excuses.dropna()\n",
        "excuses = excuses.rename(columns={\"EID\":\"eid\",\n",
        "                                  \"absent\":\"absence_code\"})\n",
        "excuses[\"date\"] = pd.to_datetime(excuses[\"date\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrcGYoMqAW_O"
      },
      "source": [
        "# scores\n",
        "scores = scores.rename(columns={\"EID\":\"eid\",\n",
        "                                \"Date\":\"date\",\n",
        "                                \"Score\":\"score\",\n",
        "                                \"Cases\":\"cases\"})\n",
        "sc = scores.groupby([\"eid\",\"date\"], as_index=False).agg({\"score\":np.mean,\"cases\":np.size})\n",
        "sc[\"date\"] = pd.to_datetime(sc[\"date\"])\n",
        "sc[\"score\"] = round(sc[\"score\"].astype(\"float64\"),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ZiTsbwIXZo"
      },
      "source": [
        "# Completion Data\n",
        "comp = sessions.rename(columns={\"bamboo_id\":\"eid\",\n",
        "                            \"project_name\":\"project\"})\n",
        "# Correct Datatypes\n",
        "# To get the dates working: basically, the date and time need to be converted to strings and then concatenated, from there, a UTC suffix can be added with the first tz_localize, converted to Pacific time, and then remove the suffix with the second tz_localize\n",
        "comp[\"vc_login\"] = pd.to_datetime(comp[\"LOGIN_DATE\"].astype(str) + \" \" + comp['lOGIN_TIME'].astype(str)).dt.tz_localize(\"US/Pacific\").dt.tz_localize(None)\n",
        "comp[\"LOGOUT_DATE\"] = comp[\"LOGOUT_DATE\"].fillna(\"2021-10-14\")\n",
        "comp['LOGOUT_TIME'] = comp['LOGOUT_TIME'].fillna(\"10:50:00\")\n",
        "comp[\"vc_logout\"] = pd.to_datetime(comp[\"LOGOUT_DATE\"].astype(str) + \" \" + comp['LOGOUT_TIME'].astype(str)).dt.tz_localize(\"US/Pacific\").dt.tz_localize(None)\n",
        "comp[\"date\"] = comp[\"vc_login\"].dt.date\n",
        "comp[\"eid\"] = comp[\"eid\"].fillna(0)\n",
        "#comp = comp[comp[\"eid\"] != '26045B']\n",
        "#comp = comp[comp[\"eid\"] != 'jleejoyce']\n",
        "#comp = comp[comp[\"eid\"] != 'kbigelow']\n",
        "#comp = comp[comp[\"eid\"] != 'tstrauss']\n",
        "#comp[\"eid\"] = comp[\"eid\"].astype(\"str\").astype(\"int64\")\n",
        "#comp = comp.groupby([\"eid\",\"project\",\"date\"],as_index=False).agg({\"completes\":sum,\n",
        "#                                                                \"calls\":sum})\n",
        "comp[\"date\"] = pd.to_datetime(comp[\"date\"])\n",
        "comp[\"project\"] = comp[\"project\"].str.replace('_','-').str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK5kyVO5WaIp"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpf4xH0lWYCc"
      },
      "source": [
        "################################################################### Schedule\n",
        "# The entire table is based on who should be here, which is determined by the schedule\n",
        "metrics = sched[[\"eid\",\"sched_project\",\"date\",\"sched_start\",\"sched_end\"]]\n",
        "\n",
        "################################################################### Employee\n",
        "# Add employee information\n",
        "metrics = pd.merge(metrics,employee,on=[\"eid\"],how=\"left\")\n",
        "\n",
        "################################################################### Exceptions\n",
        "# Add in how many exceptions they filed\n",
        "metrics = pd.merge(metrics,excep,on=[\"eid\",\"date\"],how=\"left\").fillna(0)\n",
        "\n",
        "################################################################### Voxco\n",
        "# First Voxco Login\n",
        "# Figure out the first voxco login by person/date\n",
        "firstVC = sesh.groupby([\"eid\",\"date\"],as_index=False).agg({\"vc_login\":min})\n",
        "# prepare the list of projects\n",
        "vcProjectIn = sesh[[\"eid\",\"date\",\"vc_login\",\"vc_project\" ]]\n",
        "# rename project\n",
        "vcProjectIn = vcProjectIn.rename(columns={\"vc_project\":\"vc_login_project\"})\n",
        "# Join the project to the first login of the day so you only have the project for the first login\n",
        "firstVC = pd.merge(firstVC,vcProjectIn,on=[\"eid\",\"date\",\"vc_login\"],how=\"left\")\n",
        "# convert firstVC date to proper format\n",
        "firstVC[\"date\"] = pd.to_datetime(firstVC[\"date\"])\n",
        "# Join first login info\n",
        "metrics = pd.merge(metrics,firstVC,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# Last Voxco Logout\n",
        "# Last logout \n",
        "lastVC = sesh.groupby([\"eid\",\"date\"],as_index=False).agg({\"vc_logout\":max})\n",
        "# Last logout project\n",
        "vcProjectOut = sesh[[\"eid\",\"date\",\"vc_logout\",\"vc_project\" ]]\n",
        "# rename project\n",
        "vcProjectOut = vcProjectOut.rename(columns={\"vc_project\":\"vc_logout_project\"})\n",
        "# Join project to logout\n",
        "lastVC = pd.merge(lastVC,vcProjectOut,on=[\"eid\",\"date\",\"vc_logout\"],how=\"left\")\n",
        "# convert lastVC date to proper format\n",
        "lastVC[\"date\"] = pd.to_datetime(lastVC[\"date\"])\n",
        "# Join last logout info\n",
        "metrics = pd.merge(metrics,lastVC,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# Voxco Durations\n",
        "# sum up durations, project agnostic to fit the table\n",
        "vcTimes = sesh.groupby([\"eid\",\"date\"],as_index=False).agg({\"sesh_dur\":sum,\n",
        "                                                           \"soat\":sum,\n",
        "                                                           \"talk_time\":sum,\n",
        "                                                           \"waiting_time\":sum,\n",
        "                                                           \"pause_time\":sum,\n",
        "                                                           \"review_time\":sum})\n",
        "# Convert vcTimes date to correct format\n",
        "vcTimes[\"date\"] = pd.to_datetime(vcTimes[\"date\"])\n",
        "# Join durations\n",
        "metrics = pd.merge(metrics,vcTimes,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# Voxco Count\n",
        "# count the sessions\n",
        "vcCount = sesh.groupby([\"eid\",\"date\"],as_index=False)[\"eid\"].size()\n",
        "# Convert vcCount date to proper format\n",
        "vcCount[\"date\"] = pd.to_datetime(vcCount[\"date\"])\n",
        "vcCount = vcCount.rename(columns={\"size\":\"vc_session_count\"})\n",
        "# Join count\n",
        "metrics = pd.merge(metrics,vcCount,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "################################################################### Slices\n",
        "# First OnTime Punch\n",
        "# Figure out the first OnTime Punch by person/date\n",
        "firstOT = sl.groupby([\"eid\",\"date\"],as_index=False).agg({\"ontime_start\":min})\n",
        "# prepare the list of projects\n",
        "otProjectIn = sl[[\"eid\",\"date\",\"ontime_start\",\"ontime_project\" ]]\n",
        "# rename project\n",
        "otProjectIn = otProjectIn.rename(columns={\"ontime_project\":\"ontime_start_project\"})\n",
        "# Join the project to the first punch of the day so you only have the project for the first punch\n",
        "firstOT = pd.merge(firstOT,otProjectIn,on=[\"eid\",\"date\",\"ontime_start\"],how=\"left\")\n",
        "# convert firstVC date to proper format\n",
        "firstOT[\"date\"] = pd.to_datetime(firstOT[\"date\"])\n",
        "# Join first login info\n",
        "metrics = pd.merge(metrics,firstOT,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# Last OnTime Punch\n",
        "# Figure out the last OnTime Punch by person/date\n",
        "lastOT = sl.groupby([\"eid\",\"date\"],as_index=False).agg({\"ontime_end\":max})\n",
        "# prepare the list of projects\n",
        "otProjectOut = sl[[\"eid\",\"date\",\"ontime_end\",\"ontime_project\" ]]\n",
        "# rename project\n",
        "otProjectOut = otProjectOut.rename(columns={\"ontime_project\":\"ontime_end_project\"})\n",
        "# Join the project to the first punch of the day so you only have the project for the first punch\n",
        "lastOT = pd.merge(lastOT,otProjectOut,on=[\"eid\",\"date\",\"ontime_end\"],how=\"left\")\n",
        "# convert firstVC date to proper format\n",
        "lastOT[\"date\"] = pd.to_datetime(lastOT[\"date\"])\n",
        "# Join first login info\n",
        "metrics = pd.merge(metrics,lastOT,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# Ontime Durations\n",
        "## This is unreliable because the list of ontime slices may include defunct corrected slices\n",
        "## Usze the project table instead\n",
        "# sum up durations, project agnostic to fit the table\n",
        "#otTimes = sl.groupby([\"eid\",\"date\"],as_index=False).agg({\"ontime_duration\":sum})\n",
        "# Convert otTimes date to correct format\n",
        "#otTimes[\"date\"] = pd.to_datetime(otTimes[\"date\"])\n",
        "# Join durations\n",
        "#metrics = pd.merge(metrics,otTimes,on=[\"eid\",\"date\"],how=\"left\")\n",
        "metrics = pd.merge(metrics,ontMin,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# OT Count\n",
        "# count the sessions\n",
        "otCount = sl.groupby([\"eid\",\"date\"],as_index=False)[\"eid\"].size()\n",
        "# Convert otCount date to proper format\n",
        "otCount[\"date\"] = pd.to_datetime(otCount[\"date\"])\n",
        "otCount = otCount.rename(columns={\"size\":\"ontime_slice_count\"})\n",
        "# Join count\n",
        "metrics = pd.merge(metrics,otCount,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "# OnTime Breaks\n",
        "breakData = sl[sl[\"punch\"]==\"break\"]\n",
        "breakCount = breakData.groupby([\"eid\",\"date\"],as_index=False).size()\n",
        "breakSum = breakData.groupby([\"eid\",\"date\"],as_index=False).agg({\"ontime_duration\":sum})\n",
        "breakData = pd.merge(breakCount,breakSum,on=[\"eid\",\"date\"],how=\"left\")\n",
        "breakData = breakData.rename(columns={\"ontime_duration\":\"break_duration\",\n",
        "                                      \"size\":\"break_count\"})\n",
        "breakData[\"date\"] = pd.to_datetime(breakData[\"date\"])\n",
        "metrics = pd.merge(metrics,breakData,on=[\"eid\",\"date\"],how=\"left\")\n",
        "\n",
        "################################################################### Availability\n",
        "metrics = pd.merge(metrics,avail,on=[\"date\",\"eid\"],how=\"left\")\n",
        "\n",
        "################################################################### Excuses\n",
        "metrics = pd.merge(metrics,excuses,on=[\"date\",\"eid\"],how=\"left\")\n",
        "\n",
        "################################################################### Monitoring\n",
        "metrics = pd.merge(metrics,sc,on=[\"date\",\"eid\"],how=\"left\")\n",
        "\n",
        "################################################################### MOAR COLUMNS!!!\n",
        "# Diff between schedule start and ontime start\n",
        "metrics = metrics.assign(schedule_to_ontime=metrics[\"ontime_start\"] - metrics[\"sched_start\"])\n",
        "metrics[\"schedule_to_ontime\"] = metrics[\"schedule_to_ontime\"].dt.total_seconds()\n",
        "metrics[\"schedule_to_ontime\"] = metrics[\"schedule_to_ontime\"] / 60\n",
        "# Diff between ontime start and voxco start\n",
        "metrics = metrics.assign(ontime_to_voxco=metrics[\"vc_login\"] - metrics[\"ontime_start\"])\n",
        "metrics[\"ontime_to_voxco\"] = metrics[\"ontime_to_voxco\"].dt.total_seconds()\n",
        "metrics[\"ontime_to_voxco\"] = metrics[\"ontime_to_voxco\"]/60\n",
        "# Diff between voxco end and ontime end\n",
        "metrics = metrics.assign(voxco_to_ontime=metrics[\"ontime_end\"] - metrics[\"vc_logout\"])\n",
        "metrics[\"voxco_to_ontime\"] = metrics[\"voxco_to_ontime\"].dt.total_seconds()\n",
        "metrics[\"voxco_to_ontime\"] = metrics[\"voxco_to_ontime\"]/60\n",
        "# Diff between ontime end and schedule end\n",
        "metrics = metrics.assign(ontime_to_schedule=metrics[\"sched_end\"] - metrics[\"ontime_end\"])\n",
        "metrics[\"ontime_to_schedule\"] = metrics[\"ontime_to_schedule\"].dt.total_seconds()\n",
        "metrics[\"ontime_to_schedule\"] = metrics[\"ontime_to_schedule\"] /60\n",
        "\n",
        "# \"sesh_dur\" to mins\n",
        "metrics[\"sesh_dur\"] = metrics[\"sesh_dur\"]/60\n",
        "# \"soat\" to mins\n",
        "metrics[\"soat\"] = metrics[\"soat\"]/60\n",
        "# \"talk_time\" to mins\n",
        "metrics[\"talk_time\"] = metrics[\"talk_time\"]/60\n",
        "# \"waiting_time\" to mins\n",
        "metrics[\"waiting_time\"] = metrics[\"waiting_time\"]/60\n",
        "# \"pause_time\" to mins\n",
        "metrics[\"pause_time\"] = metrics[\"pause_time\"]/60\n",
        "# \"review_time\" to mins\n",
        "metrics[\"review_time\"] = metrics[\"review_time\"]/60\n",
        "# \"schedule_duration\" to mins\n",
        "metrics = metrics.assign(schedule_duration=metrics[\"sched_end\"] - metrics[\"sched_start\"])\n",
        "metrics[\"schedule_duration\"] = metrics[\"schedule_duration\"].dt.total_seconds()\n",
        "metrics[\"schedule_duration\"] = metrics[\"schedule_duration\"] / 60\n",
        "# Work Rate\n",
        "metrics = metrics.assign(work_rate=metrics[\"ontime_duration\"] / metrics[\"schedule_duration\"])\n",
        "# Absence Rate\n",
        "metrics = metrics.assign(absence_rate=metrics[\"ontime_duration\"] / (metrics[\"schedule_duration\"])-1)\n",
        "# Potential Discrepancy\n",
        "metrics = metrics.assign(potential_discrepancy=metrics[\"schedule_duration\"] - metrics[\"sesh_dur\"])\n",
        "\n",
        "\n",
        "################################################################### Reorder table\n",
        "metrics = metrics[[\"full_name\",\n",
        "                   \"eid\",\n",
        "                   \"Job Title\",\n",
        "                   \"Division\",\n",
        "                   \"Status\",\n",
        "                   \"hire_date\",\n",
        "                   \"date\",\n",
        "                   \"potential_discrepancy\",\n",
        "                   \"work_rate\",\n",
        "                   \"absence_rate\",\n",
        "                   \"absence_code\",\n",
        "                   \"exception_count\",\n",
        "                   \"score\",\n",
        "                   \"cases\",\n",
        "                   \"avail_start\",\n",
        "                   \"sched_start\",\n",
        "                   \"sched_project\",\n",
        "                   \"schedule_to_ontime\",\n",
        "                   \"ontime_start\",\n",
        "                   \"ontime_start_project\",\n",
        "                   \"ontime_to_voxco\",\n",
        "                   \"vc_login\",\n",
        "                   \"vc_login_project\",\n",
        "                   \"vc_logout\",\n",
        "                   \"vc_logout_project\",\n",
        "                   \"voxco_to_ontime\",\n",
        "                   \"ontime_end\",\n",
        "                   \"ontime_end_project\",\n",
        "                   \"ontime_to_schedule\",\n",
        "                   \"sched_end\",\n",
        "                   \"avail_end\",\n",
        "                   \"sched_project\",\n",
        "                   \"talk_time\",\n",
        "                   \"waiting_time\",\n",
        "                   \"pause_time\",\n",
        "                   \"review_time\",\n",
        "                   \"sesh_dur\",\n",
        "                   \"soat\",\n",
        "                   \"ontime_duration\",\n",
        "                   \"schedule_duration\",\n",
        "                   \"vc_session_count\",\n",
        "                   \"ontime_slice_count\",\n",
        "                   \"break_duration\",\n",
        "                   \"break_count\"]]\n",
        "\n",
        "# Pre-filter the list. Only RAs, only CRD, no Staff, no trainees\n",
        "\n",
        "#print(metrics.dtypes)\n",
        "m = metrics[(metrics[\"Job Title\"]==\"Research Associate\")&(metrics[\"Division\"]==\"CRD\")]\n",
        "#data_table.DataTable(metrics)\n",
        "#data_table.DataTable(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7oTUk3fhRjR"
      },
      "source": [
        "# Concern List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w56urH1yhVuu"
      },
      "source": [
        "# This code block produces a list of concerning circumstances for Workforce Management to address\n",
        "# The first part establishes some parameters for the concerns list\n",
        "# The second part creates the list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Parameters\n",
        "#How many exceptions trigger an event?\n",
        "exception_trigger = 2\n",
        "# What defines a concerning discrepancy?\n",
        "## Greater than the absolute value of one standard deviation from the mean?\n",
        "discrepancy_trigger = 30\n",
        "# Work Rate\n",
        "work_rate_trigger = 0.7\n",
        "# Schedule to Ontime\n",
        "sched_to_ontime_trigger = 5\n",
        "# Lunch Length\n",
        "lunch_length = 30\n",
        "# Lunch Count per 8 hours\n",
        "lunch_count = 1\n",
        "# Break Length\n",
        "break_length = 30\n",
        "# Break Count\n",
        "break_count = 2\n",
        "# Required break time \n",
        "requiredBreakTolerance = 0.05 # in hours\n",
        "\n",
        "# Flagger\n",
        "\n",
        "## Exceptions\n",
        "ex = m[m[\"exception_count\"] >= exception_trigger]\n",
        "ex[\"exception_count\"] = ex[\"exception_count\"].astype(\"int64\")\n",
        "ex = ex.assign(message=\"Employee has \"+ex[\"exception_count\"].astype(\"str\")+\" exceptions. \"+(ex[\"exception_count\"]-(exception_trigger-1)).astype(\"str\")+\" more than allowance.\")\n",
        "ex = ex[[\"full_name\",\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "# Discrepancy\n",
        "di = m[abs(m[\"potential_discrepancy\"]) > discrepancy_trigger]\n",
        "di = di.assign(message=\"Employee has \"+round(di[\"potential_discrepancy\"]).astype(\"int64\").astype(\"str\")+\" discrepancy minutes. \"+\"The parameter is \"+str(discrepancy_trigger)+\" minutes.\")\n",
        "di = di[[\"full_name\",\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "# Schedule to Ontime\n",
        "so = m[abs(m[\"schedule_to_ontime\"])>sched_to_ontime_trigger]\n",
        "so = so.assign(message=\"Employee has exceeded the \" + str(sched_to_ontime_trigger) + \" minute schedule to ontime trigger by \" + str(metrics[\"schedule_to_ontime\"] - sched_to_ontime_trigger) + \" minutes.\" )\n",
        "so = so[[\"full_name\",\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "# Ontime to Voxco\n",
        "\n",
        "\n",
        "\n",
        "# Voxco to Ontime\n",
        "\n",
        "\n",
        "\n",
        "# Ontime to Schedule\n",
        "\n",
        "\n",
        "\n",
        "# Too many breaks\n",
        "\n",
        "br = sl[[\"eid\",\"date\",\"ontime_duration\",\"punch\"]]\n",
        "br = br[(br[\"punch\"] == \"break\") | (br[\"punch\"] == \"lunch\")]\n",
        "b = br[(br[\"punch\"] == \"break\")]\n",
        "l = br[(br[\"punch\"] == \"lunch\")]\n",
        "\n",
        "l = l.groupby([\"eid\",\"date\"],as_index=False).agg({\"ontime_duration\":sum,\n",
        "                                                  \"punch\":np.size})\n",
        "ll = l[l[\"ontime_duration\"] > lunch_length]\n",
        "lc = l[l[\"punch\"] > lunch_count]\n",
        "ll = ll.assign(message=\"Lunch too long: \" + ll[\"ontime_duration\"].astype(\"str\") + \" minutes.\")\n",
        "ll = ll[[\"eid\",\"date\",\"message\"]]\n",
        "lc = lc.assign(message=\"Too many lunches: \" + lc[\"punch\"].astype(\"str\") + \" lunch slices.\")\n",
        "lc = lc[[\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "b = b.groupby([\"eid\",\"date\"],as_index=False).agg({\"ontime_duration\":sum,\n",
        "                                                  \"punch\":np.size})\n",
        "bl = b[b[\"ontime_duration\"] > break_length]\n",
        "bc = b[b[\"punch\"] > break_count]\n",
        "bl = bl.assign(message=\"break too long: \" + bl[\"ontime_duration\"].astype(\"str\") + \" minutes.\")\n",
        "bl = bl[[\"eid\",\"date\",\"message\"]]\n",
        "bc = bc.assign(message=\"Too many breaks: \" + bc[\"punch\"].astype(\"str\") + \" break slices.\")\n",
        "bc = bc[[\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "breaks = pd.concat([ll,lc,bl,bc])\n",
        "breaks = pd.merge(breaks,employee,on=[\"eid\"],how=\"left\")\n",
        "breaks = breaks[(breaks[\"Division\"]==\"CRD\")&(breaks[\"Job Title\"]==\"Research Associate\")]\n",
        "breaks = breaks[[\"full_name\",\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "# Schedule outside of availability\n",
        "\n",
        "soa = m[(m[\"sched_start\"]<m[\"avail_start\"]) | (m[\"avail_end\"]<m[\"avail_start\"])]\n",
        "soa = soa.assign(message=\"RAs schedule does not fall within availability parameters.\")\n",
        "soa = soa[[\"full_name\",\"eid\",\"date\",\"message\"]]\n",
        "\n",
        "# Missed Required Break\n",
        "mb = variance[variance[\"status\"]==\"ACTIVE\"]\n",
        "mb = mb.assign(miss=mb[\"actual-break\"]<mb[\"paid-break\"]-requiredBreakTolerance)\n",
        "mb = mb[mb[\"miss\"]==True]\n",
        "mb = mb.rename(columns={\"EID\":\"eid\"})\n",
        "mb = pd.merge(mb,employee,on=\"eid\",how=\"left\")\n",
        "mb = mb.assign(message=\"Employee has not taken enough breaks by law.\")\n",
        "mb = mb[[\"full_name\",\"eid\",\"date\",\"message\"]]\n",
        "# Concerns\n",
        "concerns = pd.concat([di,ex,so,soa,mb])\n",
        "data_table.DataTable(concerns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBJ5zHpb7lgO"
      },
      "source": [
        "# Punch List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BsrtS747n_c"
      },
      "source": [
        "# Voxco punches\n",
        "vcPunches = sesh[[\"eid\",\"vc_project\",\"date\",\"vc_login\",\"vc_logout\"]]\n",
        "vcPunches = vcPunches.assign(system=\"voxco\")\n",
        "vcPunches[\"date\"] = pd.to_datetime(vcPunches[\"date\"])\n",
        "vcPunches = vcPunches.rename(columns={\"vc_login\":\"in\",\n",
        "                                      \"vc_logout\":\"out\",\n",
        "                                      \"vc_project\":\"project\"})\n",
        "# OnTime punches\n",
        "otPunches = sl[[\"eid\",\"ontime_project\",\"date\",\"ontime_start\",\"ontime_end\", \"punch\"]]\n",
        "otPunches = otPunches.assign(system=\"ontime\")\n",
        "otPunches[\"date\"] = pd.to_datetime(otPunches[\"date\"])\n",
        "otPunches = otPunches.rename(columns={\"ontime_start\":\"in\",\n",
        "                                      \"ontime_end\":\"out\",\n",
        "                                      \"ontime_project\":\"project\",\n",
        "                                      \"punch\":\"ontime_punch\"})\n",
        "# Union\n",
        "punchList = pd.concat([vcPunches,otPunches])\n",
        "punchList = punchList.sort_values(by=[\"eid\",\"in\"])\n",
        "# Sort\n",
        "punchList = punchList[punchList[\"eid\"]!=0]\n",
        "punchList = punchList.sort_values(by = [\"eid\",\"in\"])\n",
        "# name\n",
        "punchList = pd.merge(punchList,employee,on=\"eid\",how=\"left\")\n",
        "punchList = punchList[[\"eid\",\"full_name\",\"project\",\"date\",\"in\",\"out\",\"system\",\"ontime_punch\"]]\n",
        "\n",
        "#punchList = punchList[punchList[\"date\"] == date.today() -  timedelta(days=1)]\n",
        "#diag(vcPunches,None,None)\n",
        "#data_table.DataTable(punchList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG7VtF2iiG9p"
      },
      "source": [
        "# Attendance\n",
        "att = variance[[\"EID\",\"name\",\"date\",\"department\",\"status\",\"scheduled-work\",\"actual-work\",\"schedule-status\"]]\n",
        "att = att[(att[\"status\"]==\"ACTIVE\")]\n",
        "att = att.assign(workRate=att[\"actual-work\"] / att[\"scheduled-work\"])\n",
        "att[\"workRate\"] = round(att[\"workRate\"].astype(\"float64\"),2)\n",
        "att = att.assign(alias=att[\"name\"] + \" - \" + att[\"EID\"].astype(\"str\"))\n",
        "att[\"baz\"] = pd.DataFrame.where(cond=att[\"schedule-status\"]==\"NORMAL\", self= att[\"workRate\"].astype(\"str\"), other= att[\"schedule-status\"])\n",
        "att[\"date\"] = pd.to_datetime(att[\"date\"])\n",
        "realRate = att[att[\"schedule-status\"]== \"NORMAL\"]\n",
        "\n",
        "realRate = realRate.groupby([\"alias\"], as_index = False).agg({\"workRate\": np.mean})\n",
        "\n",
        "ncns = att[(att[\"schedule-status\"]== \"NORMAL\")&(att[\"workRate\"]==0.0)]\n",
        "ncns = ncns.groupby([\"alias\"], as_index =False).size()\n",
        "ncns = ncns.rename(columns={\"size\":\"ncns\"})\n",
        "\n",
        "atp = att.pivot(index=\"alias\",columns=\"date\",values=\"baz\")\n",
        "atp = pd.merge(atp,realRate,on=\"alias\",how=\"left\")\n",
        "atp = pd.merge(atp,ncns,on=\"alias\",how=\"left\")\n",
        "\n",
        "atp[\"workRate\"] = round(atp[\"workRate\"],2)\n",
        "atp[\"ncns\"] = atp[\"ncns\"].fillna(0)\n",
        "atp[\"ncns\"] = atp[\"ncns\"].astype(\"int64\")\n",
        "atp = atp.astype(\"str\")\n",
        "#data_table.DataTable(atp)\n",
        "#print(atp.dtypes)\n",
        "#print(atp.head(50))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW8gl5PFxseW"
      },
      "source": [
        "# Attendance Version 2\n",
        "\n",
        "at = variance[[\"EID\",\"name\",\"date\",\"department\",\"status\",\"scheduled-work\",\"actual-work\",\"schedule-status\"]]\n",
        "# make a table with eid and job title so that you can \n",
        "# filter where equals \"Research Associate\"\n",
        "e = employee[[\"eid\",\"Job Title\"]]\n",
        "e = e.rename(columns={\"eid\":\"EID\"})\n",
        "at = pd.merge(at,e,on=\"EID\",how=\"left\")\n",
        "at = at[(at[\"Job Title\"]==\"Research Associate\")]\n",
        "\n",
        "at = at[(at[\"status\"]==\"ACTIVE\")]\n",
        "at[\"date\"] = pd.to_datetime(at[\"date\"])\n",
        "at = at.assign(rate=at[\"actual-work\"] / at[\"scheduled-work\"])\n",
        "at[\"rate\"] = round(at[\"rate\"],2)\n",
        "at = at.assign(alias=at[\"name\"] + \" - \" + at[\"EID\"].astype(\"str\"))\n",
        "at[\"baz\"] = pd.DataFrame.where(cond=at[\"schedule-status\"]==\"NORMAL\", self= at[\"rate\"].astype(\"str\"), other= at[\"schedule-status\"])\n",
        "piv = at.pivot(index=\"alias\",columns=\"date\",values =\"baz\")\n",
        "# JA process people who have not shown up for three days\n",
        "ja = variance[variance[\"status\"]==\"ACTIVE\"]\n",
        "ja[\"date\"] = pd.to_datetime(ja[\"date\"])\n",
        "ja = ja.assign(ranker=ja.groupby([\"EID\"])[\"date\"].rank(method=\"first\", ascending=False))\n",
        "ja = ja[(\n",
        "          ((ja[\"schedule-status\"]==\"NORMAL\")&(ja[\"actual-work\"]==0.0)&(ja[\"ranker\"]==1.0)) |\n",
        "          ((ja[\"schedule-status\"]==\"NORMAL\")&(ja[\"actual-work\"]==0.0)&(ja[\"ranker\"]==2.0)) |\n",
        "          ((ja[\"schedule-status\"]==\"NORMAL\")&(ja[\"actual-work\"]==0.0)&(ja[\"ranker\"]==3.0))\n",
        "       )]\n",
        "ja = ja.groupby([\"EID\",\"name\"],as_index=False).agg({\"ranker\":sum})\n",
        "ja = ja[ja[\"ranker\"]==6.0]\n",
        "ja = ja.assign(ja=\"ja flag\")\n",
        "ja = ja.assign(alias=ja[\"name\"] + \" - \" + ja[\"EID\"].astype(\"str\"))\n",
        "ja = ja[[\"alias\",\"ja\"]]\n",
        "ja = ja.drop_duplicates()\n",
        "# Check that the person has been scheduled for at least 23 hours in the past week\n",
        "# Try to find an excuse?\n",
        "# Run three weeks worth of work rate\n",
        "s = sched.rename(columns={\"eid\":\"EID\"})\n",
        "variance[\"date\"] = pd.to_datetime(variance[\"date\"])\n",
        "\n",
        "d = pd.merge(variance,s,on=[\"EID\",\"date\"],how=\"left\")\n",
        "\n",
        "d = d.assign(schedLen=d[\"sched_end\"]-d[\"sched_start\"])\n",
        "d[\"schedLen\"] = d[\"schedLen\"].dt.total_seconds()\n",
        "d[\"schedLen\"] = round(d[\"schedLen\"] / 60 / 60,2)\n",
        "d = d.assign(alias=d[\"name\"]+\" - \"+d[\"EID\"].astype(\"str\"))\n",
        "\n",
        "ds = d[d[\"date\"]>= pd.to_datetime(date.today()-timedelta(days=7))]\n",
        "\n",
        "ds = ds.groupby([\"alias\"],as_index=False).agg({\"schedLen\":sum})\n",
        "\n",
        "d = d.groupby([\"alias\"],as_index=False).agg({\"actual-work\":sum,\n",
        "                                              \"scheduled-work\":sum})\n",
        "d = d.assign(workRate=d[\"actual-work\"] / d[\"scheduled-work\"])\n",
        "d[\"workRate\"] = round(d[\"workRate\"],2)\n",
        "\n",
        "d = d[[\"alias\",\"workRate\"]]\n",
        "ds = ds[[\"alias\",\"schedLen\"]]\n",
        "piv = pd.merge(piv,ja,on=\"alias\",how=\"left\")\n",
        "piv = pd.merge(piv,d,on=\"alias\",how=\"left\")\n",
        "piv = pd.merge(piv,ds,on=\"alias\",how=\"left\")\n",
        "piv = piv.fillna(\"\")\n",
        "\n",
        "\n",
        "# Requested absences\n",
        "#a = at[[\"EID\",\"alias\"]]\n",
        "##a = a.rename(columns = {\"EID\":\"eid\"})\n",
        "#r = pd.merge(avail,a,on=\"eid\",how=\"left\")\n",
        "#requests = requests.rename(columns={\"detail.date\":\"date\",\n",
        "#                                    \"detail.associate\":\"eid\"})\n",
        "#requests[\"date\"] = pd.to_datetime(requests[\"date\"])\n",
        "#r = pd.merge(r,requests,on=\"eid\",how=\"right\").dropna().drop_duplicates()\n",
        "\n",
        "\n",
        "\n",
        "print(piv)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4iTBtZD2vFx"
      },
      "source": [
        "# Resource Allocation\n",
        "ra = avail.assign(hrs=avail[\"avail_end\"] - avail[\"avail_start\"])\n",
        "ra[\"hrs\"] = round(ra[\"hrs\"].dt.total_seconds() / 60 / 60,2)\n",
        "def parse_values(x):\n",
        "    if x >= 8.5:\n",
        "       return 8.5\n",
        "    else:\n",
        "       return x\n",
        "ra[\"hrs\"] = ra[\"hrs\"].apply(parse_values)\n",
        "ra = pd.merge(ra,sched,on=[\"eid\",\"date\"],how=\"left\")\n",
        "ra = ra.assign(shrs=ra[\"sched_end\"] - ra[\"sched_start\"])\n",
        "ra[\"shrs\"] = round(ra[\"shrs\"].dt.total_seconds() / 60 / 60,2)\n",
        "\n",
        "tb = ra[[\"date\",\"eid\",\"sched_project\",\"hrs\",\"shrs\"]]\n",
        "\n",
        "ra[\"hrs\"] = ra[\"hrs\"].fillna(0.0)\n",
        "ra[\"shrs\"] = ra[\"shrs\"].fillna(0.0)\n",
        "\n",
        "ra = pd.merge(ra,employee,on=\"eid\",how=\"left\")\n",
        "ra = ra[(ra[\"Status\"]==\"Active\")&(ra[\"Job Title\"]==\"Research Associate\")]\n",
        "ra = ra.rename(columns={\"hrs\":\"available hours\",\n",
        "                        \"shrs\":\"scheduled hours\"})\n",
        "ra = ra.assign(unscheduled=ra[\"available hours\"] - ra[\"scheduled hours\"])\n",
        "ra = ra[[\"date\",\"eid\",\"full_name\",\"sched_project\",\"available hours\",\"scheduled hours\",\"unscheduled\"]]\n",
        "\n",
        "\n",
        "data_table.DataTable(ra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJesUVk9NRY"
      },
      "source": [
        "# CRD Stats\n",
        "# total time in ontime versus total time in Voxco, discrepancy\n",
        "# Monitoring sessions count and score\n",
        "# call count, complete count\n",
        "\n",
        "# Employee Table\n",
        "#em = employee[employee[\"Job Title\"]==\"Research Associate\"]\n",
        "# Availability Table - total hours available\n",
        "#av = avail.assign(avail_dur=avail[\"avail_end\"] - avail[\"avail_start\"])\n",
        "#av[\"avail_dur\"] = av[\"avail_dur\"].dt.total_seconds() / 60 / 60\n",
        "# Exception Table\n",
        "#ex = excep.groupby([\"date\",\"eid\"],as_index=False).agg({\"exception_count\":sum})\n",
        "# Score Table\n",
        "#sc = scores.groupby([\"date\",\"eid\"],as_index=False).agg({\"score\":np.mean,\n",
        "#                                                        \"cases\":np.size})\n",
        "#sc[\"date\"] = pd.to_datetime(sc[\"date\"])\n",
        "# Variance Table\n",
        "#vr = variance[[\"EID\",\"date\",\"scheduled-work\",\"actual-work\",\"paid-break\",\"actual-break\",\"late-minutes\",\"schedule-status\"]]\n",
        "#vr = vr.rename(columns={\"EID\":\"eid\"})\n",
        "# Schedule Table\n",
        "#scd = sched[[\"eid\",\"date\",\"sched_start\",\"sched_end\"]]\n",
        "# Sessions Table\n",
        "#sessions[\"date\"]  = pd.to_datetime(sessions[\"LOGIN_DATE\"])\n",
        "#sessions = sessions[(sessions[\"bamboo_id\"]!=\"kbigelow\")&\n",
        "#                    (sessions[\"bamboo_id\"]!=\"26045B\")&\n",
        "#                    (sessions[\"bamboo_id\"]!=\"tstrauss\")]\n",
        "\n",
        "#sessions[\"eid\"] = sessions[\"bamboo_id\"] \n",
        "#se = sessions.groupby([\"date\",\"eid\"],as_index=False).agg({\"sesh_dur\":sum,\n",
        "#                                                          \"talkTime\":sum,\n",
        "#                                                          \"waitingTime\":sum,\n",
        "#                                                          \"pauseTime\":sum,\n",
        "#                                                          \"reviewTime\":sum,\n",
        "#                                                          \"soat\":sum)\n",
        "\n",
        "# Everything together\n",
        "#stats = pd.merge(em,av,on=\"eid\",how=\"left\")\n",
        "#stats[\"date\"] = pd.to_datetime(stats[\"date\"])\n",
        "#stats = pd.merge(stats,ex,on=[\"eid\",\"date\"],how=\"left\")\n",
        "#stats = pd.merge(stats,sc,on=[\"eid\",\"date\"],how=\"left\")\n",
        "#stats = pd.merge(stats,vr,on=[\"eid\",\"date\"],how=\"left\")\n",
        "#stats = pd.merge(stats,scd,on=[\"eid\",\"date\"],how=\"left\")\n",
        "#se[\"eid\"] = se[\"eid\"].astype(\"int64\")\n",
        "#stats = pd.merge(stats,se,on=[\"date\",\"eid\"],how=\"left\")\n",
        "\n",
        "# Varibles\n",
        "# Week ago\n",
        "#seven_days = date.today()-timedelta(days=7)\n",
        "\n",
        "# New Columns\n",
        "#df['new column name'] = df['column name'].apply(lambda x: 'value if condition is met' if x condition else 'value if condition is not met')\n",
        "#stats[\"new_hire\"] = stats[\"hire_date\"].apply(lambda x: \"yes\" if x >= seven_days else \"no\")\n",
        "#stats = stats.assign(sched_len=stats[\"sched_end\"] - stats[\"sched_start\"])\n",
        "#stats[\"sched_len\"] = stats[\"sched_len\"].dt.total_seconds() / 60 / 60\n",
        "\n",
        "# Aggregation\n",
        "\n",
        "#e = stats.groupby([\"full_name\",\"eid\",\"Status\",\"Division\",\"Job Title\",\"new_hire\"])\n",
        "\n",
        "#print(stats[stats[\"eid\"]==65])\n",
        "#data_table.DataTable(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PZ6o1C_z0ri"
      },
      "source": [
        "proj = project.groupby([\"EID\",\"project\",\"date\"],as_index=False).agg({\"hours\":sum})\n",
        "proj = proj.rename(columns={\"EID\":\"eid\"})\n",
        "proj[\"hours\"] = round(proj[\"hours\"],2)\n",
        "vox = sesh.groupby([\"eid\",\"vc_project\",\"date\"],as_index=False).agg({\"sesh_dur\":sum})\n",
        "vox = vox.rename(columns={\"vc_project\":\"project\"})\n",
        "vox[\"sesh_dur\"] = round(vox[\"sesh_dur\"] / 60 / 60,2)\n",
        "vox[\"project\"] = vox[\"project\"].str.lower().str.replace(\"_\",\"-\")\n",
        "vox[\"date\"] = pd.to_datetime(vox[\"date\"])\n",
        "ov = pd.merge(proj,vox,on=[\"eid\",\"date\",\"project\"],how=\"left\")\n",
        "ov = ov.rename(columns = {\"hours\":\"ontime\",\n",
        "                          \"sesh_dur\":\"voxco\"})\n",
        "ov = ov.assign(diff=ov[\"ontime\"] - ov[\"voxco\"])\n",
        "ov[\"diff\"] = round(ov[\"diff\"],2)\n",
        "ee = employee[[\"eid\",\"full_name\",\"Job Title\"]]\n",
        "ov = pd.merge(ov,ee,on=\"eid\",how=\"left\")\n",
        "ov[\"projecta\"] = ov[\"project\"].str.contains(\"cf\",case=False,regex=False)\n",
        "ov = ov[ov[\"projecta\"]==True]\n",
        "ov = ov[ov[\"Job Title\"]==\"Research Associate\"]\n",
        "\n",
        "ov = ov.sort_values(by=[\"full_name\",\"date\"])\n",
        "ov = ov[[\"eid\",\"full_name\",\"date\",\"project\",\"ontime\",\"voxco\",\"diff\"]]\n",
        "data_table.DataTable(ov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BngBo7RdcPeN"
      },
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn_ZU4V5uVa0"
      },
      "source": [
        "# Performance manager report\n",
        "from datetime import datetime, timedelta\n",
        "#if date.today().weekday() < 6:\n",
        "#  punchList = punchList[punchList[\"date\"]== datetime.today() - timedelta(days=1)]\n",
        "#  flags = flags[flags[\"date\"]== datetime.today() - timedelta(days=1)]\n",
        "#else:\n",
        "#  punchList = punchList[punchList[\"date\"]>= datetime.today() - timedelta(days=3)]\n",
        "#  flags = flags[flags[\"date\"]>= datetime.today() - timedelta(days=3)]\n",
        "\n",
        "flags = concerns.drop_duplicates() #pd.concat([di, breaks])\n",
        "with pd.ExcelWriter('CF_Performance_Manager_Report '+ date.today().strftime(\"%d-%b-%Y\") +'.xlsx',\n",
        "                    datetime_format = 'YYYY-MM-DD HH:MM:SS') as writer:\n",
        "                    punchList.to_excel(writer, sheet_name = 'Punch List', index=False)\n",
        "                    flags.to_excel(writer, sheet_name = 'Flags', index=False)\n",
        "                    piv.to_excel(writer, sheet_name = 'Attendance', index=False)\n",
        "                    ra.to_excel(writer, sheet_name = 'schedule projection', index=False)\n",
        "                    #stats.to_excel(writer, sheet_name = 'Statistics', index=False)\n",
        "                    ov.to_excel(writer, sheet_name = \"discrepancy\", index = False)\n",
        "files.download('CF_Performance_Manager_Report '+ date.today().strftime(\"%d-%b-%Y\") +'.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0iekvaH2vHh"
      },
      "source": [
        "# Data Visualizations"
      ]
    }
  ]
}